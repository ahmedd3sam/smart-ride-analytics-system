# S3-Kafka-Flink-Pipeline Project Structure

## Overview
This project implements a real-time streaming data pipeline that orchestrates data flow from AWS S3 â†’ Kafka â†’ Flink â†’ PostgreSQL â†’ Grafana using Apache Airflow for orchestration.

## Directory Structure

```
s3-kafka-flink-pipeline/
â”‚
â”œâ”€â”€ ğŸ“ dags/                          # Airflow DAGs (Directed Acyclic Graphs)
â”‚   â”œâ”€â”€ taxi_pipeline_dag.py         # Main orchestration DAG
â”‚   â””â”€â”€ orch.py                      # (Empty/backup file)
â”‚
â”œâ”€â”€ ğŸ“ producer/                      # S3 to Kafka Producer Service
â”‚   â”œâ”€â”€ dockerfile                    # Docker image definition for producer
â”‚   â”œâ”€â”€ requirements.txt              # Python dependencies (boto3, kafka-python, pandas, pyarrow)
â”‚   â”œâ”€â”€ s3_to_kafka.py               # Main producer script (reads S3, sends to Kafka)
â”‚   â””â”€â”€ venv/                        # Python virtual environment (local development)
â”‚
â”œâ”€â”€ ğŸ“ jobs/                          # Flink Processing Jobs
â”‚   â””â”€â”€ kafka_consumer_ui.py         # Flink PyFlink job (consumes Kafka, transforms, loads to Postgres)
â”‚
â”œâ”€â”€ ğŸ“ lib/                           # Flink Connector Libraries (JAR files)
â”‚   â”œâ”€â”€ flink-connector-jdbc-3.1.2-1.17.jar      # JDBC connector for PostgreSQL
â”‚   â”œâ”€â”€ flink-connector-kafka-1.17.2.jar        # Kafka connector
â”‚   â”œâ”€â”€ flink-sql-connector-kafka-1.17.2.jar    # Kafka SQL connector
â”‚   â”œâ”€â”€ kafka_2.12-3.5.1.jar                    # Kafka client library
â”‚   â”œâ”€â”€ kafka-clients-3.5.1.jar                 # Kafka clients
â”‚   â”œâ”€â”€ postgresql-42.6.0.jar                   # PostgreSQL JDBC driver
â”‚   â””â”€â”€ lib/                          # Nested lib directory (duplicates)
â”‚
â”œâ”€â”€ ğŸ“ logs/                          # Airflow Execution Logs
â”‚   â”œâ”€â”€ dag_id=taxi_streaming_pipeline/  # Logs organized by DAG and run ID
â”‚   â””â”€â”€ scheduler/                    # Airflow scheduler logs
â”‚
â”œâ”€â”€ ğŸ“ connect/                       # (Appears empty - possibly for Kafka Connect)
â”‚
â”œâ”€â”€ ğŸ“ flink-job/                     # (Appears empty - possibly for Flink job artifacts)
â”‚
â”œâ”€â”€ ğŸ“„ docker-compose.yml             # Docker Compose configuration
â”‚   â””â”€â”€ Defines all services:
â”‚       - zookeeper (Kafka dependency)
â”‚       - kafka (message broker)
â”‚       - postgres (data warehouse)
â”‚       - flink-jobmanager (Flink cluster manager)
â”‚       - flink-taskmanager (Flink worker)
â”‚       - s3-producer (data producer container)
â”‚       - grafana (visualization)
â”‚       - airflow-webserver (Airflow UI)
â”‚       - airflow-scheduler (Airflow scheduler)
â”‚
â”œâ”€â”€ ğŸ“„ Dockerfile-airflow             # Custom Airflow image with Docker client
â”‚
â”œâ”€â”€ ğŸ“„ fix_docker_socket.sh          # Helper script to fix Docker socket mounting
â”‚
â””â”€â”€ ğŸ“„ *.jar                          # Root-level JAR files (duplicates of lib/)
    â”œâ”€â”€ flink-connector-jdbc-3.1.2-1.17.jar
    â”œâ”€â”€ flink-connector-kafka-1.17.2.jar
    â”œâ”€â”€ flink-sql-connector-kafka-1.17.2.jar
    â”œâ”€â”€ kafka-clients-3.6.0.jar
    â””â”€â”€ postgresql-42.6.0.jar
```

## Component Details

### 1. **dags/** - Airflow Orchestration
- **Purpose**: Contains Airflow DAG definitions that orchestrate the entire pipeline
- **Files**:
  - `taxi_pipeline_dag.py`: Main DAG with tasks:
    - Service readiness checks (Kafka, Flink, Postgres)
    - S3 to Kafka producer execution
    - Flink job submission
    - Data verification

### 2. **producer/** - Data Producer Service
- **Purpose**: Reads data from AWS S3 and publishes to Kafka
- **Components**:
  - `s3_to_kafka.py`: 
    - Connects to AWS S3 using boto3
    - Reads Parquet files
    - Converts to JSON and streams to Kafka topic `s3-taxi-trips`
  - `dockerfile`: Builds Python 3.12 image with dependencies
  - `requirements.txt`: boto3, kafka-python, pandas, pyarrow, numpy

### 3. **jobs/** - Flink Processing Jobs
- **Purpose**: Contains Flink streaming jobs for data transformation
- **Files**:
  - `kafka_consumer_ui.py`: PyFlink job that:
    - Consumes from Kafka topic `s3-taxi-trips`
    - Cleans and validates data
    - Enriches with calculated fields (trip duration, speed, fare metrics)
    - Loads transformed data into PostgreSQL `taxi_trips_dashboard` table

### 4. **lib/** - Flink Connector Libraries
- **Purpose**: JAR files required by Flink for connectors
- **Libraries**:
  - Kafka connectors (for consuming from Kafka)
  - JDBC connector (for writing to PostgreSQL)
  - PostgreSQL driver (database connectivity)
  - Kafka client libraries

### 5. **logs/** - Airflow Execution Logs
- **Purpose**: Stores execution logs for each DAG run
- **Structure**: Organized by DAG ID, run ID, and task ID
- **Usage**: Debugging and monitoring pipeline execution

### 6. **docker-compose.yml** - Infrastructure Definition
- **Purpose**: Defines all services and their configurations
- **Services**:
  - **Zookeeper**: Required by Kafka for coordination
  - **Kafka**: Message broker (port 9092)
  - **PostgreSQL**: Database (port 5432, database: taxi_db)
  - **Flink JobManager**: Flink cluster coordinator (port 8081)
  - **Flink TaskManager**: Flink worker nodes
  - **s3-producer**: Producer container (built from ./producer)
  - **Grafana**: Visualization dashboard (port 3000)
  - **Airflow**: Web UI (port 8085) and scheduler

## Data Flow

```
AWS S3 (Parquet files)
    â†“
[Producer Container] - s3_to_kafka.py
    â†“
Kafka Topic: s3-taxi-trips
    â†“
[Flink Job] - kafka_consumer_ui.py
    â†“ (transformation & enrichment)
PostgreSQL Table: taxi_trips_dashboard
    â†“
Grafana Dashboard (visualization)
```

## Key Configuration Files

1. **docker-compose.yml**: 
   - Service definitions
   - Network configuration
   - Volume mounts (dags, jobs, lib)
   - Environment variables

2. **Dockerfile-airflow**:
   - Custom Airflow image
   - Includes Docker client for container management
   - Python dependencies (psycopg2)

3. **producer/dockerfile**:
   - Python 3.12 base image
   - Installs producer dependencies
   - Sets CMD to run s3_to_kafka.py

## Volume Mounts (in docker-compose.yml)

- `./dags` â†’ `/opt/airflow/dags` (Airflow DAGs)
- `./jobs` â†’ `/opt/flink/jobs` (Flink job files)
- `./lib` â†’ `/opt/flink/usrlib` (Flink connector libraries)
- `/var/run/docker.sock` â†’ `/var/run/docker.sock` (Docker socket for Airflow)

## Network Architecture

All containers run on the same Docker network (created by docker-compose), allowing them to communicate using service names:
- `kafka:9092` - Kafka broker
- `postgres:5432` - PostgreSQL database
- `flink-jobmanager:8081` - Flink JobManager
- `flink-taskmanager` - Flink TaskManager

## Port Mappings

- **8085** â†’ Airflow Web UI
- **8081** â†’ Flink Web UI
- **3000** â†’ Grafana
- **9092** â†’ Kafka
- **5432** â†’ PostgreSQL
- **2181** â†’ Zookeeper

## Execution Flow

1. **Airflow DAG triggers** â†’ `taxi_pipeline_dag.py`
2. **Service checks** â†’ Verify Kafka, Flink, Postgres are ready
3. **Producer task** â†’ Starts s3-producer container, reads S3, sends to Kafka
4. **Flink job task** â†’ Submits Flink job to consume, transform, and load data
5. **Verification task** â†’ Checks data in PostgreSQL
6. **Grafana** â†’ Visualizes the data from PostgreSQL

