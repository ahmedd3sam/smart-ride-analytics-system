[2025-11-27T08:59:58.367+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: taxi_streaming_pipeline.start_kafka_producer manual__2025-11-27T08:59:15.125749+00:00 [queued]>
[2025-11-27T08:59:58.376+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: taxi_streaming_pipeline.start_kafka_producer manual__2025-11-27T08:59:15.125749+00:00 [queued]>
[2025-11-27T08:59:58.376+0000] {taskinstance.py:1359} INFO - Starting attempt 2 of 2
[2025-11-27T08:59:58.387+0000] {taskinstance.py:1380} INFO - Executing <Task(BashOperator): start_kafka_producer> on 2025-11-27 08:59:15.125749+00:00
[2025-11-27T08:59:58.391+0000] {standard_task_runner.py:57} INFO - Started process 139 to run task
[2025-11-27T08:59:58.394+0000] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'taxi_streaming_pipeline', 'start_kafka_producer', 'manual__2025-11-27T08:59:15.125749+00:00', '--job-id', '90', '--raw', '--subdir', 'DAGS_FOLDER/taxi_pipeline_dag.py', '--cfg-path', '/tmp/tmpnpknkztr']
[2025-11-27T08:59:58.396+0000] {standard_task_runner.py:85} INFO - Job 90: Subtask start_kafka_producer
[2025-11-27T08:59:58.450+0000] {task_command.py:415} INFO - Running <TaskInstance: taxi_streaming_pipeline.start_kafka_producer manual__2025-11-27T08:59:15.125749+00:00 [running]> on host 485e0d4eae03
[2025-11-27T08:59:58.543+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Menna' AIRFLOW_CTX_DAG_ID='taxi_streaming_pipeline' AIRFLOW_CTX_TASK_ID='start_kafka_producer' AIRFLOW_CTX_EXECUTION_DATE='2025-11-27T08:59:15.125749+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-11-27T08:59:15.125749+00:00'
[2025-11-27T08:59:58.545+0000] {subprocess.py:63} INFO - Tmp dir root location: /tmp
[2025-11-27T08:59:58.546+0000] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', '\n        set -e  # Exit on error\n        set -o pipefail  # Exit on pipe errors\n        \n        echo "=========================================="\n        echo "Starting S3 to Kafka Producer"\n        echo "=========================================="\n        \n        # Check if docker command is available and has socket access\n        DOCKER_AVAILABLE=0\n        if command -v docker &> /dev/null; then\n            echo "Checking Docker access..."\n            if docker ps &> /dev/null 2>&1; then\n                DOCKER_AVAILABLE=1\n                echo "✓ Docker is available and accessible"\n                docker --version\n            else\n                echo "✗ Docker command exists but socket is not accessible"\n                echo "Error details:"\n                docker ps 2>&1 | head -5 || true\n            fi\n        else\n            echo "✗ Docker command not found"\n        fi\n        \n        if [ "$DOCKER_AVAILABLE" != "1" ]; then\n            echo ""\n            echo "ERROR: Docker is not available. Cannot run producer container."\n            echo "Please ensure Docker socket is mounted and accessible."\n            exit 1\n        fi\n        \n        # Check if Kafka is ready (producer depends on it)\n        echo ""\n        echo "Checking if Kafka is ready..."\n        KAFKA_READY=0\n        if docker ps --filter "name=^kafka$" -q | grep -q .; then\n            echo "✓ Kafka container is running"\n            KAFKA_READY=1\n        else\n            echo "✗ Kafka container is not running"\n            echo "Please ensure Kafka is started: docker-compose up -d kafka"\n            exit 1\n        fi\n        \n        # Find docker-compose.yml directory\n        echo ""\n        echo "Locating docker-compose.yml..."\n        COMPOSE_DIR=""\n        for dir in /opt/airflow/dags/.. /root/s3-kafka-flink-pipeline $HOME/s3-kafka-flink-pipeline /home/menna/s3-kafka-flink-pipeline; do\n            if [ -f "$dir/docker-compose.yml" ]; then\n                COMPOSE_DIR="$dir"\n                echo "✓ Found docker-compose.yml at: $COMPOSE_DIR"\n                break\n            fi\n        done\n        \n        if [ -z "$COMPOSE_DIR" ]; then\n            echo "✗ Error: Could not find docker-compose.yml file"\n            echo "Searched in:"\n            echo "  /opt/airflow/dags/.."\n            echo "  /root/s3-kafka-flink-pipeline"\n            echo "  $HOME/s3-kafka-flink-pipeline"\n            echo "  /home/menna/s3-kafka-flink-pipeline"\n            exit 1\n        fi\n        \n        # Check container status\n        echo ""\n        echo "Checking s3-producer container status..."\n        CONTAINER_EXISTS=$(docker ps -a --filter "name=^s3-producer$" -q | wc -l)\n        CONTAINER_RUNNING=$(docker ps --filter "name=^s3-producer$" -q | wc -l)\n        \n        if [ "$CONTAINER_RUNNING" = "1" ]; then\n            echo "Container is already running. Waiting for it to complete..."\n            EXIT_CODE=$(docker wait s3-producer)\n            echo "Container completed with exit code: $EXIT_CODE"\n        else\n            # Remove old container if it exists (stopped/exited)\n            if [ "$CONTAINER_EXISTS" = "1" ]; then\n                echo "Removing old s3-producer container..."\n                docker rm s3-producer 2>/dev/null || true\n            fi\n            \n            # Create and start fresh container\n            echo ""\n            echo "Creating and starting fresh s3-producer container..."\n            cd "$COMPOSE_DIR"\n            \n            if ! command -v docker-compose &> /dev/null; then\n                echo "✗ Error: docker-compose command not available"\n                echo "Trying \'docker compose\' (newer syntax)..."\n                if command -v docker &> /dev/null && docker compose version &> /dev/null; then\n                    COMPOSE_CMD="docker compose"\n                else\n                    echo "✗ Neither \'docker-compose\' nor \'docker compose\' available"\n                    exit 1\n                fi\n            else\n                COMPOSE_CMD="docker-compose"\n            fi\n            \n            echo "Using: $COMPOSE_CMD"\n            echo "Starting container..."\n            \n            # Start the container\n            if $COMPOSE_CMD up -d s3-producer; then\n                echo "✓ Container started successfully"\n            else\n                EXIT_CODE=$?\n                echo "✗ Failed to start container (exit code: $EXIT_CODE)"\n                echo "Checking container logs..."\n                docker logs s3-producer 2>&1 | tail -20 || true\n                exit $EXIT_CODE\n            fi\n            \n            # Wait a moment for container to initialize\n            echo "Waiting for container to initialize..."\n            sleep 3\n            \n            # Check if container is still running (it might exit quickly)\n            if docker ps --filter "name=^s3-producer$" -q | grep -q .; then\n                echo "Container is running. Waiting for script to complete..."\n                EXIT_CODE=$(docker wait s3-producer)\n                echo "Container exited with code: $EXIT_CODE"\n            else\n                # Container already exited, get exit code using a method that avoids Jinja2 issues\n                # Use docker inspect without format, then parse JSON with grep/sed\n                CONTAINER_INFO=$(docker inspect s3-producer 2>/dev/null)\n                if [ -n "$CONTAINER_INFO" ]; then\n                    # Extract exit code from JSON using grep and sed (avoids Jinja2 template syntax)\n                    EXIT_CODE=$(echo "$CONTAINER_INFO" | grep -o \'"ExitCode":[0-9]*\' | grep -o \'[0-9]*\' | head -1)\n                    if [ -z "$EXIT_CODE" ]; then\n                        EXIT_CODE=1  # Default to error if can\'t determine\n                    fi\n                else\n                    EXIT_CODE=1  # Container doesn\'t exist, assume error\n                fi\n                echo "Container already exited with code: $EXIT_CODE"\n            fi\n        fi\n        \n        # Check exit code and show logs if failed\n        echo ""\n        if [ "$EXIT_CODE" != "0" ]; then\n            echo "=========================================="\n            echo "ERROR: Producer failed with exit code $EXIT_CODE"\n            echo "=========================================="\n            echo ""\n            echo "Container logs:"\n            echo "----------------------------------------"\n            docker logs s3-producer 2>&1 | tail -50\n            echo "----------------------------------------"\n            echo ""\n            echo "Troubleshooting:"\n            echo "1. Check if S3 credentials are correct"\n            echo "2. Verify S3 bucket and file exist"\n            echo "3. Check if Kafka is accessible from producer container"\n            echo "4. View full logs: docker logs s3-producer"\n            exit $EXIT_CODE\n        else\n            echo "=========================================="\n            echo "✓ Producer completed successfully!"\n            echo "=========================================="\n            echo ""\n            echo "Container logs (last 20 lines):"\n            echo "----------------------------------------"\n            docker logs s3-producer 2>&1 | tail -20\n            echo "----------------------------------------"\n            echo ""\n            echo "Data has been sent to Kafka topic: s3-taxi-trips"\n        fi\n        ']
[2025-11-27T08:59:58.556+0000] {subprocess.py:86} INFO - Output:
[2025-11-27T08:59:58.558+0000] {subprocess.py:93} INFO - ==========================================
[2025-11-27T08:59:58.558+0000] {subprocess.py:93} INFO - Starting S3 to Kafka Producer
[2025-11-27T08:59:58.558+0000] {subprocess.py:93} INFO - ==========================================
[2025-11-27T08:59:58.558+0000] {subprocess.py:93} INFO - Checking Docker access...
[2025-11-27T08:59:58.629+0000] {subprocess.py:93} INFO - ✓ Docker is available and accessible
[2025-11-27T08:59:58.643+0000] {subprocess.py:93} INFO - Docker version 29.0.4, build 3247a5a
[2025-11-27T08:59:58.644+0000] {subprocess.py:93} INFO - 
[2025-11-27T08:59:58.644+0000] {subprocess.py:93} INFO - Checking if Kafka is ready...
[2025-11-27T08:59:58.680+0000] {subprocess.py:93} INFO - ✓ Kafka container is running
[2025-11-27T08:59:58.680+0000] {subprocess.py:93} INFO - 
[2025-11-27T08:59:58.680+0000] {subprocess.py:93} INFO - Locating docker-compose.yml...
[2025-11-27T08:59:58.680+0000] {subprocess.py:93} INFO - ✗ Error: Could not find docker-compose.yml file
[2025-11-27T08:59:58.680+0000] {subprocess.py:93} INFO - Searched in:
[2025-11-27T08:59:58.680+0000] {subprocess.py:93} INFO -   /opt/airflow/dags/..
[2025-11-27T08:59:58.680+0000] {subprocess.py:93} INFO -   /root/s3-kafka-flink-pipeline
[2025-11-27T08:59:58.680+0000] {subprocess.py:93} INFO -   /root/s3-kafka-flink-pipeline
[2025-11-27T08:59:58.681+0000] {subprocess.py:93} INFO -   /home/menna/s3-kafka-flink-pipeline
[2025-11-27T08:59:58.681+0000] {subprocess.py:97} INFO - Command exited with return code 1
[2025-11-27T08:59:58.693+0000] {taskinstance.py:1935} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/bash.py", line 210, in execute
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-11-27T08:59:58.696+0000] {taskinstance.py:1398} INFO - Marking task as FAILED. dag_id=taxi_streaming_pipeline, task_id=start_kafka_producer, execution_date=20251127T085915, start_date=20251127T085958, end_date=20251127T085958
[2025-11-27T08:59:58.706+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 90 for task start_kafka_producer (Bash command failed. The command returned a non-zero exit code 1.; 139)
[2025-11-27T08:59:58.727+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2025-11-27T08:59:58.749+0000] {taskinstance.py:2776} INFO - 0 downstream tasks scheduled from follow-on schedule check
