[2025-11-27T10:40:59.104+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: taxi_streaming_pipeline.run_flink_job manual__2025-11-27T10:39:50.371542+00:00 [queued]>
[2025-11-27T10:40:59.119+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: taxi_streaming_pipeline.run_flink_job manual__2025-11-27T10:39:50.371542+00:00 [queued]>
[2025-11-27T10:40:59.119+0000] {taskinstance.py:1359} INFO - Starting attempt 2 of 2
[2025-11-27T10:40:59.141+0000] {taskinstance.py:1380} INFO - Executing <Task(BashOperator): run_flink_job> on 2025-11-27 10:39:50.371542+00:00
[2025-11-27T10:40:59.150+0000] {standard_task_runner.py:57} INFO - Started process 824 to run task
[2025-11-27T10:40:59.155+0000] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'taxi_streaming_pipeline', 'run_flink_job', 'manual__2025-11-27T10:39:50.371542+00:00', '--job-id', '193', '--raw', '--subdir', 'DAGS_FOLDER/taxi_pipeline_dag.py', '--cfg-path', '/tmp/tmpmdh1la9z']
[2025-11-27T10:40:59.157+0000] {standard_task_runner.py:85} INFO - Job 193: Subtask run_flink_job
[2025-11-27T10:40:59.239+0000] {task_command.py:415} INFO - Running <TaskInstance: taxi_streaming_pipeline.run_flink_job manual__2025-11-27T10:39:50.371542+00:00 [running]> on host 485e0d4eae03
[2025-11-27T10:40:59.361+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Menna' AIRFLOW_CTX_DAG_ID='taxi_streaming_pipeline' AIRFLOW_CTX_TASK_ID='run_flink_job' AIRFLOW_CTX_EXECUTION_DATE='2025-11-27T10:39:50.371542+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-11-27T10:39:50.371542+00:00'
[2025-11-27T10:40:59.363+0000] {subprocess.py:63} INFO - Tmp dir root location: /tmp
[2025-11-27T10:40:59.364+0000] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', '\n        echo "=========================================="\n        echo "Submitting Flink Job"\n        echo "=========================================="\n\n        # Check if Docker is available\n        if ! command -v docker &> /dev/null; then\n            echo "ERROR: Docker command not available"\n            exit 1\n        fi\n\n        JOB_MANAGER_NAME="flink-jobmanager"\n        TASK_MANAGER_NAME="flink-taskmanager"\n        \n        # Verify JobManager is running\n        if ! docker ps --filter "name=^${JOB_MANAGER_NAME}$" -q | grep -q .; then\n            echo "ERROR: Flink JobManager container not running"\n            docker ps || true\n            exit 1\n        fi\n\n        # Verify TaskManager is running and ready\n        echo "Checking Flink TaskManager availability..."\n        if ! docker ps --filter "name=^${TASK_MANAGER_NAME}$" -q | grep -q .; then\n            echo "ERROR: Flink TaskManager container not running"\n            docker ps || true\n            exit 1\n        fi\n\n        echo "✓ Flink JobManager is running"\n        echo "✓ Flink TaskManager is running"\n\n        # Wait for Flink cluster to be fully ready\n        echo "Waiting for Flink cluster to be ready..."\n        MAX_RETRIES=30\n        RETRY_COUNT=0\n        FLINK_READY=0\n\n        while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do\n            # Check if Flink REST API is accessible\n            if docker exec "$JOB_MANAGER_NAME" curl -s http://localhost:8081/overview > /dev/null 2>&1; then\n                # Check if TaskManager is registered\n                TASK_MANAGERS=$(docker exec "$JOB_MANAGER_NAME" curl -s http://localhost:8081/taskmanagers 2>/dev/null | grep -o "taskmanagers" | wc -l || echo "0")\n                if [ "$TASK_MANAGERS" -gt "0" ]; then\n                    echo "✓ Flink cluster is ready (TaskManagers registered)"\n                    FLINK_READY=1\n                    break\n                fi\n            fi\n            RETRY_COUNT=$((RETRY_COUNT + 1))\n            echo "Waiting for Flink cluster... ($RETRY_COUNT/$MAX_RETRIES)"\n            sleep 2\n        done\n\n        if [ "$FLINK_READY" != "1" ]; then\n            echo "ERROR: Flink cluster not ready after $MAX_RETRIES retries"\n            echo "JobManager logs:"\n            docker logs "$JOB_MANAGER_NAME" 2>&1 | tail -20\n            exit 1\n        fi\n\n        # Wait a bit more for Kafka messages to be available\n        echo "Waiting for Kafka messages to be available..."\n        sleep 10\n\n        JOB_FILE="/opt/flink/jobs/kafka_consumer_ui.py"\n        echo "Verifying Flink job file: $JOB_FILE"\n        if ! docker exec "$JOB_MANAGER_NAME" test -f "$JOB_FILE"; then\n            echo "ERROR: Flink job file not found: $JOB_FILE"\n            echo "Files in /opt/flink/jobs:"\n            docker exec "$JOB_MANAGER_NAME" ls -la /opt/flink/jobs/ || true\n            exit 1\n        fi\n\n        # Verify the job file structure\n        echo "Verifying Flink job file structure..."\n        \n        # Check for env.execute()\n        if docker exec "$JOB_MANAGER_NAME" grep -q "^env.execute" "$JOB_FILE"; then\n            echo "✓ env.execute() is present"\n        else\n            echo "WARNING: env.execute() not found, attempting to uncomment..."\n            docker exec "$JOB_MANAGER_NAME" sed -i \'s/^# env.execute/env.execute/\' "$JOB_FILE" 2>/dev/null || true\n        fi\n        \n        # Check for INSERT INTO statement (creates the operator)\n        if docker exec "$JOB_MANAGER_NAME" grep -q "INSERT INTO" "$JOB_FILE"; then\n            echo "✓ INSERT INTO statement found (creates operator)"\n        else\n            echo "ERROR: INSERT INTO statement not found in job file!"\n            echo "The Flink job must have an INSERT INTO statement to create operators."\n            exit 1\n        fi\n        \n        # Check for source table definition\n        if docker exec "$JOB_MANAGER_NAME" grep -q "CREATE TABLE.*taxi_trips" "$JOB_FILE"; then\n            echo "✓ Source table definition found"\n        else\n            echo "WARNING: Source table definition may be missing"\n        fi\n        \n        # Check for sink table definition\n        if docker exec "$JOB_MANAGER_NAME" grep -q "CREATE TABLE.*taxi_trips_dashboard" "$JOB_FILE"; then\n            echo "✓ Sink table definition found"\n        else\n            echo "WARNING: Sink table definition may be missing"\n        fi\n\n        echo ""\n        echo "Submitting Flink job..."\n        echo "Command: flink run -py $JOB_FILE -d"\n        \n        # Submit the job\n        if docker exec "$JOB_MANAGER_NAME" flink run -py "$JOB_FILE" -d; then\n            echo ""\n            echo "✓ Flink job submitted successfully!"\n            echo "Job is running and will:"\n            echo "  - Consume from Kafka topic: s3-taxi-trips"\n            echo "  - Transform and enrich the data"\n            echo "  - Load into PostgreSQL table: taxi_trips_dashboard"\n            echo ""\n            echo "Check Flink UI at http://localhost:8081 for job status"\n        else\n            EXIT_CODE=$?\n            echo ""\n            echo "ERROR: Flink job submission failed (exit code: $EXIT_CODE)"\n            echo ""\n            echo "Flink JobManager logs (last 50 lines):"\n            echo "----------------------------------------"\n            docker logs "$JOB_MANAGER_NAME" 2>&1 | tail -50\n            echo "----------------------------------------"\n            echo ""\n            echo "Flink TaskManager logs (last 30 lines):"\n            echo "----------------------------------------"\n            docker logs "$TASK_MANAGER_NAME" 2>&1 | tail -30\n            echo "----------------------------------------"\n            echo ""\n            echo "Troubleshooting:"\n            echo "1. Check if Kafka has messages: docker exec kafka kafka-console-consumer --bootstrap-server localhost:9092 --topic s3-taxi-trips --from-beginning --max-messages 1"\n            echo "2. Check Flink UI: http://localhost:8081"\n            echo "3. Verify connectors are available in /opt/flink/usrlib/"\n            exit $EXIT_CODE\n        fi\n        ']
[2025-11-27T10:40:59.375+0000] {subprocess.py:86} INFO - Output:
[2025-11-27T10:40:59.378+0000] {subprocess.py:93} INFO - ==========================================
[2025-11-27T10:40:59.378+0000] {subprocess.py:93} INFO - Submitting Flink Job
[2025-11-27T10:40:59.378+0000] {subprocess.py:93} INFO - ==========================================
[2025-11-27T10:40:59.429+0000] {subprocess.py:93} INFO - Checking Flink TaskManager availability...
[2025-11-27T10:40:59.484+0000] {subprocess.py:93} INFO - ✓ Flink JobManager is running
[2025-11-27T10:40:59.485+0000] {subprocess.py:93} INFO - ✓ Flink TaskManager is running
[2025-11-27T10:40:59.485+0000] {subprocess.py:93} INFO - Waiting for Flink cluster to be ready...
[2025-11-27T10:40:59.817+0000] {subprocess.py:93} INFO - ✓ Flink cluster is ready (TaskManagers registered)
[2025-11-27T10:40:59.817+0000] {subprocess.py:93} INFO - Waiting for Kafka messages to be available...
[2025-11-27T10:41:09.820+0000] {subprocess.py:93} INFO - Verifying Flink job file: /opt/flink/jobs/kafka_consumer_ui.py
[2025-11-27T10:41:09.946+0000] {subprocess.py:93} INFO - Verifying Flink job file structure...
[2025-11-27T10:41:10.116+0000] {subprocess.py:93} INFO - ✓ env.execute() is present
[2025-11-27T10:41:10.257+0000] {subprocess.py:93} INFO - ✓ INSERT INTO statement found (creates operator)
[2025-11-27T10:41:10.406+0000] {subprocess.py:93} INFO - ✓ Source table definition found
[2025-11-27T10:41:10.557+0000] {subprocess.py:93} INFO - ✓ Sink table definition found
[2025-11-27T10:41:10.558+0000] {subprocess.py:93} INFO - 
[2025-11-27T10:41:10.560+0000] {subprocess.py:93} INFO - Submitting Flink job...
[2025-11-27T10:41:10.562+0000] {subprocess.py:93} INFO - Command: flink run -py /opt/flink/jobs/kafka_consumer_ui.py -d
[2025-11-27T10:41:18.799+0000] {subprocess.py:93} INFO - WARNING: An illegal reflective access operation has occurred
[2025-11-27T10:41:18.799+0000] {subprocess.py:93} INFO - WARNING: Illegal reflective access by org.apache.flink.api.java.ClosureCleaner (file:/opt/flink/lib/flink-dist-1.17.2.jar) to field java.lang.String.value
[2025-11-27T10:41:18.799+0000] {subprocess.py:93} INFO - WARNING: Please consider reporting this to the maintainers of org.apache.flink.api.java.ClosureCleaner
[2025-11-27T10:41:18.799+0000] {subprocess.py:93} INFO - WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
[2025-11-27T10:41:18.800+0000] {subprocess.py:93} INFO - WARNING: All illegal access operations will be denied in a future release
[2025-11-27T10:41:20.140+0000] {subprocess.py:93} INFO - Job has been submitted with JobID 25f973ea87d59a5a5944154a19bcfe15
[2025-11-27T10:41:20.176+0000] {subprocess.py:93} INFO - Traceback (most recent call last):
[2025-11-27T10:41:20.177+0000] {subprocess.py:93} INFO -   File "/opt/flink/jobs/kafka_consumer_ui.py", line 286, in <module>
[2025-11-27T10:41:20.177+0000] {subprocess.py:93} INFO -     env.execute("Flink Kafka Taxi Dashboard Job")
[2025-11-27T10:41:20.177+0000] {subprocess.py:93} INFO -   File "/opt/flink/opt/python/pyflink.zip/pyflink/datastream/stream_execution_environment.py", line 763, in execute
[2025-11-27T10:41:20.178+0000] {subprocess.py:93} INFO -   File "/opt/flink/opt/python/pyflink.zip/pyflink/datastream/stream_execution_environment.py", line 1012, in _generate_stream_graph
[2025-11-27T10:41:20.178+0000] {subprocess.py:93} INFO -   File "/opt/flink/opt/python/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
[2025-11-27T10:41:20.178+0000] {subprocess.py:93} INFO -   File "/opt/flink/opt/python/pyflink.zip/pyflink/util/exceptions.py", line 146, in deco
[2025-11-27T10:41:20.178+0000] {subprocess.py:93} INFO -   File "/opt/flink/opt/python/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
[2025-11-27T10:41:20.178+0000] {subprocess.py:93} INFO - py4j.protocol.Py4JJavaError: An error occurred while calling o8.getStreamGraph.
[2025-11-27T10:41:20.179+0000] {subprocess.py:93} INFO - : java.lang.IllegalStateException: No operators defined in streaming topology. Cannot execute.
[2025-11-27T10:41:20.179+0000] {subprocess.py:93} INFO - 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.getStreamGraphGenerator(StreamExecutionEnvironment.java:2290)
[2025-11-27T10:41:20.179+0000] {subprocess.py:93} INFO - 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.getStreamGraph(StreamExecutionEnvironment.java:2257)
[2025-11-27T10:41:20.179+0000] {subprocess.py:93} INFO - 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.getStreamGraph(StreamExecutionEnvironment.java:2248)
[2025-11-27T10:41:20.179+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2025-11-27T10:41:20.179+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
[2025-11-27T10:41:20.179+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
[2025-11-27T10:41:20.179+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.reflect.Method.invoke(Unknown Source)
[2025-11-27T10:41:20.179+0000] {subprocess.py:93} INFO - 	at org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2025-11-27T10:41:20.180+0000] {subprocess.py:93} INFO - 	at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
[2025-11-27T10:41:20.180+0000] {subprocess.py:93} INFO - 	at org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)
[2025-11-27T10:41:20.180+0000] {subprocess.py:93} INFO - 	at org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2025-11-27T10:41:20.180+0000] {subprocess.py:93} INFO - 	at org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)
[2025-11-27T10:41:20.180+0000] {subprocess.py:93} INFO - 	at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)
[2025-11-27T10:41:20.180+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Unknown Source)
[2025-11-27T10:41:20.180+0000] {subprocess.py:93} INFO - 
[2025-11-27T10:41:20.255+0000] {subprocess.py:93} INFO - org.apache.flink.client.program.ProgramAbortException: java.lang.RuntimeException: Python process exits with code: 1
[2025-11-27T10:41:20.256+0000] {subprocess.py:93} INFO - 	at org.apache.flink.client.python.PythonDriver.main(PythonDriver.java:140)
[2025-11-27T10:41:20.256+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2025-11-27T10:41:20.256+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
[2025-11-27T10:41:20.256+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
[2025-11-27T10:41:20.256+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.reflect.Method.invoke(Unknown Source)
[2025-11-27T10:41:20.256+0000] {subprocess.py:93} INFO - 	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:355)
[2025-11-27T10:41:20.256+0000] {subprocess.py:93} INFO - 	at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:222)
[2025-11-27T10:41:20.257+0000] {subprocess.py:93} INFO - 	at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:105)
[2025-11-27T10:41:20.257+0000] {subprocess.py:93} INFO - 	at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:851)
[2025-11-27T10:41:20.257+0000] {subprocess.py:93} INFO - 	at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:245)
[2025-11-27T10:41:20.257+0000] {subprocess.py:93} INFO - 	at org.apache.flink.client.cli.CliFrontend.parseAndRun(CliFrontend.java:1095)
[2025-11-27T10:41:20.257+0000] {subprocess.py:93} INFO - 	at org.apache.flink.client.cli.CliFrontend.lambda$mainInternal$9(CliFrontend.java:1189)
[2025-11-27T10:41:20.257+0000] {subprocess.py:93} INFO - 	at org.apache.flink.runtime.security.contexts.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:28)
[2025-11-27T10:41:20.257+0000] {subprocess.py:93} INFO - 	at org.apache.flink.client.cli.CliFrontend.mainInternal(CliFrontend.java:1189)
[2025-11-27T10:41:20.258+0000] {subprocess.py:93} INFO - 	at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1157)
[2025-11-27T10:41:20.258+0000] {subprocess.py:93} INFO - Caused by: java.lang.RuntimeException: Python process exits with code: 1
[2025-11-27T10:41:20.258+0000] {subprocess.py:93} INFO - 	at org.apache.flink.client.python.PythonDriver.main(PythonDriver.java:130)
[2025-11-27T10:41:20.258+0000] {subprocess.py:93} INFO - 	... 14 more
[2025-11-27T10:41:20.337+0000] {subprocess.py:93} INFO - 
[2025-11-27T10:41:20.337+0000] {subprocess.py:93} INFO - ERROR: Flink job submission failed (exit code: 31)
[2025-11-27T10:41:20.338+0000] {subprocess.py:93} INFO - 
[2025-11-27T10:41:20.338+0000] {subprocess.py:93} INFO - Flink JobManager logs (last 50 lines):
[2025-11-27T10:41:20.338+0000] {subprocess.py:93} INFO - ----------------------------------------
[2025-11-27T10:41:20.448+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-11-27T10:41:20.449+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-11-27T10:41:20.449+0000] {subprocess.py:93} INFO - 	ssl.truststore.*** = null
[2025-11-27T10:41:20.449+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-11-27T10:41:20.449+0000] {subprocess.py:93} INFO - 
[2025-11-27T10:41:20.450+0000] {subprocess.py:93} INFO - 2025-11-27 10:41:20,066 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Connecting to ResourceManager akka.tcp://flink@flink-jobmanager:6123/user/rpc/resourcemanager_*(00000000000000000000000000000000)
[2025-11-27T10:41:20.450+0000] {subprocess.py:93} INFO - 2025-11-27 10:41:20,068 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Resolved ResourceManager address, beginning registration
[2025-11-27T10:41:20.450+0000] {subprocess.py:93} INFO - 2025-11-27 10:41:20,069 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registering job manager 00000000000000000000000000000000@akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_6 for job 25f973ea87d59a5a5944154a19bcfe15.
[2025-11-27T10:41:20.450+0000] {subprocess.py:93} INFO - 2025-11-27 10:41:20,069 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registered job manager 00000000000000000000000000000000@akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_6 for job 25f973ea87d59a5a5944154a19bcfe15.
[2025-11-27T10:41:20.450+0000] {subprocess.py:93} INFO - 2025-11-27 10:41:20,071 WARN  org.apache.flink.kafka.shaded.org.apache.kafka.clients.admin.AdminClientConfig [] - The configuration 'key.deserializer' was supplied but isn't a known config.
[2025-11-27T10:41:20.451+0000] {subprocess.py:93} INFO - 2025-11-27 10:41:20,072 WARN  org.apache.flink.kafka.shaded.org.apache.kafka.clients.admin.AdminClientConfig [] - The configuration 'value.deserializer' was supplied but isn't a known config.
[2025-11-27T10:41:20.451+0000] {subprocess.py:93} INFO - 2025-11-27 10:41:20,072 WARN  org.apache.flink.kafka.shaded.org.apache.kafka.clients.admin.AdminClientConfig [] - The configuration 'enable.auto.commit' was supplied but isn't a known config.
[2025-11-27T10:41:20.451+0000] {subprocess.py:93} INFO - 2025-11-27 10:41:20,073 WARN  org.apache.flink.kafka.shaded.org.apache.kafka.clients.admin.AdminClientConfig [] - The configuration 'client.id.prefix' was supplied but isn't a known config.
[2025-11-27T10:41:20.451+0000] {subprocess.py:93} INFO - 2025-11-27 10:41:20,073 WARN  org.apache.flink.kafka.shaded.org.apache.kafka.clients.admin.AdminClientConfig [] - The configuration 'group.id' was supplied but isn't a known config.
[2025-11-27T10:41:20.451+0000] {subprocess.py:93} INFO - 2025-11-27 10:41:20,073 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - JobManager successfully registered at ResourceManager, leader id: 00000000000000000000000000000000.
[2025-11-27T10:41:20.452+0000] {subprocess.py:93} INFO - 2025-11-27 10:41:20,073 WARN  org.apache.flink.kafka.shaded.org.apache.kafka.clients.admin.AdminClientConfig [] - The configuration 'partition.discovery.interval.ms' was supplied but isn't a known config.
[2025-11-27T10:41:20.453+0000] {subprocess.py:93} INFO - 2025-11-27 10:41:20,073 WARN  org.apache.flink.kafka.shaded.org.apache.kafka.clients.admin.AdminClientConfig [] - The configuration 'auto.offset.reset' was supplied but isn't a known config.
[2025-11-27T10:41:20.453+0000] {subprocess.py:93} INFO - 2025-11-27 10:41:20,073 INFO  org.apache.flink.kafka.shaded.org.apache.kafka.common.utils.AppInfoParser [] - Kafka version: unknown
[2025-11-27T10:41:20.454+0000] {subprocess.py:93} INFO - 2025-11-27 10:41:20,073 INFO  org.apache.flink.kafka.shaded.org.apache.kafka.common.utils.AppInfoParser [] - Kafka commitId: unknown
[2025-11-27T10:41:20.454+0000] {subprocess.py:93} INFO - 2025-11-27 10:41:20,073 INFO  org.apache.flink.kafka.shaded.org.apache.kafka.common.utils.AppInfoParser [] - Kafka startTimeMs: 1764240080073
[2025-11-27T10:41:20.454+0000] {subprocess.py:93} INFO - 2025-11-27 10:41:20,074 WARN  org.apache.flink.kafka.shaded.org.apache.kafka.common.utils.AppInfoParser [] - Error registering AppInfo mbean
[2025-11-27T10:41:20.454+0000] {subprocess.py:93} INFO - javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=flink-group-enumerator-admin-client
[2025-11-27T10:41:20.454+0000] {subprocess.py:93} INFO - 	at com.sun.jmx.mbeanserver.Repository.addMBean(Unknown Source) ~[?:?]
[2025-11-27T10:41:20.455+0000] {subprocess.py:93} INFO - 	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(Unknown Source) ~[?:?]
[2025-11-27T10:41:20.455+0000] {subprocess.py:93} INFO - 	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(Unknown Source) ~[?:?]
[2025-11-27T10:41:20.455+0000] {subprocess.py:93} INFO - 	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(Unknown Source) ~[?:?]
[2025-11-27T10:41:20.455+0000] {subprocess.py:93} INFO - 	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(Unknown Source) ~[?:?]
[2025-11-27T10:41:20.456+0000] {subprocess.py:93} INFO - 	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(Unknown Source) ~[?:?]
[2025-11-27T10:41:20.456+0000] {subprocess.py:93} INFO - 	at org.apache.flink.kafka.shaded.org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:64) ~[flink-sql-connector-kafka-1.17.2.jar:1.17.2]
[2025-11-27T10:41:20.456+0000] {subprocess.py:93} INFO - 	at org.apache.flink.kafka.shaded.org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:597) ~[flink-sql-connector-kafka-1.17.2.jar:1.17.2]
[2025-11-27T10:41:20.456+0000] {subprocess.py:93} INFO - 	at org.apache.flink.kafka.shaded.org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:539) ~[flink-sql-connector-kafka-1.17.2.jar:1.17.2]
[2025-11-27T10:41:20.457+0000] {subprocess.py:93} INFO - 	at org.apache.flink.kafka.shaded.org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:478) ~[flink-sql-connector-kafka-1.17.2.jar:1.17.2]
[2025-11-27T10:41:20.457+0000] {subprocess.py:93} INFO - 	at org.apache.flink.kafka.shaded.org.apache.kafka.clients.admin.Admin.create(Admin.java:133) ~[flink-sql-connector-kafka-1.17.2.jar:1.17.2]
[2025-11-27T10:41:20.457+0000] {subprocess.py:93} INFO - 	at org.apache.flink.kafka.shaded.org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:39) ~[flink-sql-connector-kafka-1.17.2.jar:1.17.2]
[2025-11-27T10:41:20.457+0000] {subprocess.py:93} INFO - 	at org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator.getKafkaAdminClient(KafkaSourceEnumerator.java:410) ~[flink-sql-connector-kafka-1.17.2.jar:1.17.2]
[2025-11-27T10:41:20.457+0000] {subprocess.py:93} INFO - 	at org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator.start(KafkaSourceEnumerator.java:151) ~[flink-sql-connector-kafka-1.17.2.jar:1.17.2]
[2025-11-27T10:41:20.457+0000] {subprocess.py:93} INFO - 	at org.apache.flink.runtime.source.coordinator.SourceCoordinator.lambda$start$1(SourceCoordinator.java:229) ~[flink-dist-1.17.2.jar:1.17.2]
[2025-11-27T10:41:20.458+0000] {subprocess.py:93} INFO - 	at org.apache.flink.runtime.source.coordinator.SourceCoordinator.lambda$runInEventLoop$10(SourceCoordinator.java:465) ~[flink-dist-1.17.2.jar:1.17.2]
[2025-11-27T10:41:20.458+0000] {subprocess.py:93} INFO - 	at org.apache.flink.util.ThrowableCatchingRunnable.run(ThrowableCatchingRunnable.java:40) [flink-dist-1.17.2.jar:1.17.2]
[2025-11-27T10:41:20.458+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source) [?:?]
[2025-11-27T10:41:20.458+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.FutureTask.run(Unknown Source) [?:?]
[2025-11-27T10:41:20.458+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(Unknown Source) [?:?]
[2025-11-27T10:41:20.458+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) [?:?]
[2025-11-27T10:41:20.459+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) [?:?]
[2025-11-27T10:41:20.459+0000] {subprocess.py:93} INFO - 	at java.lang.Thread.run(Unknown Source) [?:?]
[2025-11-27T10:41:20.459+0000] {subprocess.py:93} INFO - 2025-11-27 10:41:20,074 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job 25f973ea87d59a5a5944154a19bcfe15: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=1}]
[2025-11-27T10:41:20.459+0000] {subprocess.py:93} INFO - 2025-11-27 10:41:20,075 INFO  org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator [] - Starting the KafkaSourceEnumerator for consumer group flink-group without periodic partition discovery.
[2025-11-27T10:41:20.459+0000] {subprocess.py:93} INFO - 2025-11-27 10:41:20,087 INFO  org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator [] - Discovered new partitions: [s3-taxi-trips-0]
[2025-11-27T10:41:20.459+0000] {subprocess.py:93} INFO - 2025-11-27 10:41:20,174 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: taxi_trips[1] -> Calc[2] -> Sink: taxi_trips_dashboard[3] (1/1) (5ccddf79ff5cdefc788dc53d4f3ad620_cbc357ccb763df2852fee8c4fc7d55f2_0_0) switched from SCHEDULED to DEPLOYING.
[2025-11-27T10:41:20.460+0000] {subprocess.py:93} INFO - 2025-11-27 10:41:20,175 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Source: taxi_trips[1] -> Calc[2] -> Sink: taxi_trips_dashboard[3] (1/1) (attempt #0) with attempt id 5ccddf79ff5cdefc788dc53d4f3ad620_cbc357ccb763df2852fee8c4fc7d55f2_0_0 and vertex id cbc357ccb763df2852fee8c4fc7d55f2_0 to 172.18.0.8:43107-93805c @ flink-taskmanager.s3-kafka-flink-pipeline_default (dataPort=35499) with allocation id 666e2ff1d9045a04f6410592e47a6b57
[2025-11-27T10:41:20.460+0000] {subprocess.py:93} INFO - ----------------------------------------
[2025-11-27T10:41:20.460+0000] {subprocess.py:93} INFO - 
[2025-11-27T10:41:20.460+0000] {subprocess.py:93} INFO - Flink TaskManager logs (last 30 lines):
[2025-11-27T10:41:20.461+0000] {subprocess.py:93} INFO - ----------------------------------------
[2025-11-27T10:41:20.533+0000] {subprocess.py:93} INFO - 2025-11-27 10:40:26,472 WARN  org.apache.flink.kafka.shaded.org.apache.kafka.clients.consumer.ConsumerConfig [] - The configuration 'partition.discovery.interval.ms' was supplied but isn't a known config.
[2025-11-27T10:41:20.534+0000] {subprocess.py:93} INFO - 2025-11-27 10:40:26,472 INFO  org.apache.flink.kafka.shaded.org.apache.kafka.common.utils.AppInfoParser [] - Kafka version: unknown
[2025-11-27T10:41:20.534+0000] {subprocess.py:93} INFO - 2025-11-27 10:40:26,473 INFO  org.apache.flink.kafka.shaded.org.apache.kafka.common.utils.AppInfoParser [] - Kafka commitId: unknown
[2025-11-27T10:41:20.534+0000] {subprocess.py:93} INFO - 2025-11-27 10:40:26,473 INFO  org.apache.flink.kafka.shaded.org.apache.kafka.common.utils.AppInfoParser [] - Kafka startTimeMs: 1764240026472
[2025-11-27T10:41:20.534+0000] {subprocess.py:93} INFO - 2025-11-27 10:40:26,475 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Starting split fetcher 0
[2025-11-27T10:41:20.535+0000] {subprocess.py:93} INFO - 2025-11-27 10:40:26,476 INFO  org.apache.flink.kafka.shaded.org.apache.kafka.clients.consumer.KafkaConsumer [] - [Consumer clientId=flink-group-0, groupId=flink-group] Subscribed to partition(s): s3-taxi-trips-0
[2025-11-27T10:41:20.535+0000] {subprocess.py:93} INFO - 2025-11-27 10:40:26,476 INFO  org.apache.flink.kafka.shaded.org.apache.kafka.clients.consumer.internals.SubscriptionState [] - [Consumer clientId=flink-group-0, groupId=flink-group] Seeking to EARLIEST offset of partition s3-taxi-trips-0
[2025-11-27T10:41:20.535+0000] {subprocess.py:93} INFO - 2025-11-27 10:40:26,483 INFO  org.apache.flink.kafka.shaded.org.apache.kafka.clients.Metadata [] - [Consumer clientId=flink-group-0, groupId=flink-group] Resetting the last seen epoch of partition s3-taxi-trips-0 to 0 since the associated topicId changed from null to QoPw1HFpSQGYPepTdzaZPw
[2025-11-27T10:41:20.535+0000] {subprocess.py:93} INFO - 2025-11-27 10:40:26,483 INFO  org.apache.flink.kafka.shaded.org.apache.kafka.clients.Metadata [] - [Consumer clientId=flink-group-0, groupId=flink-group] Cluster ID: CPNtkNp6SgeW1oSIi61Zmg
[2025-11-27T10:41:20.535+0000] {subprocess.py:93} INFO - 2025-11-27 10:40:26,487 INFO  org.apache.flink.kafka.shaded.org.apache.kafka.clients.consumer.internals.SubscriptionState [] - [Consumer clientId=flink-group-0, groupId=flink-group] Resetting offset for partition s3-taxi-trips-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-11-27T10:41:20.535+0000] {subprocess.py:93} INFO - 2025-11-27 10:41:20,141 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request 666e2ff1d9045a04f6410592e47a6b57 for job 25f973ea87d59a5a5944154a19bcfe15 from resource manager with leader id 00000000000000000000000000000000.
[2025-11-27T10:41:20.536+0000] {subprocess.py:93} INFO - 2025-11-27 10:41:20,141 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for 666e2ff1d9045a04f6410592e47a6b57.
[2025-11-27T10:41:20.536+0000] {subprocess.py:93} INFO - 2025-11-27 10:41:20,142 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Add job 25f973ea87d59a5a5944154a19bcfe15 for job leader monitoring.
[2025-11-27T10:41:20.536+0000] {subprocess.py:93} INFO - 2025-11-27 10:41:20,142 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Try to register at job manager akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_6 with leader id 00000000-0000-0000-0000-000000000000.
[2025-11-27T10:41:20.536+0000] {subprocess.py:93} INFO - 2025-11-27 10:41:20,153 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Resolved JobManager address, beginning registration
[2025-11-27T10:41:20.536+0000] {subprocess.py:93} INFO - 2025-11-27 10:41:20,169 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Successful registration at job manager akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_6 for job 25f973ea87d59a5a5944154a19bcfe15.
[2025-11-27T10:41:20.536+0000] {subprocess.py:93} INFO - 2025-11-27 10:41:20,169 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Establish JobManager connection for job 25f973ea87d59a5a5944154a19bcfe15.
[2025-11-27T10:41:20.537+0000] {subprocess.py:93} INFO - 2025-11-27 10:41:20,170 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Offer reserved slots to the leader of job 25f973ea87d59a5a5944154a19bcfe15.
[2025-11-27T10:41:20.537+0000] {subprocess.py:93} INFO - 2025-11-27 10:41:20,190 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 666e2ff1d9045a04f6410592e47a6b57.
[2025-11-27T10:41:20.537+0000] {subprocess.py:93} INFO - 2025-11-27 10:41:20,191 INFO  org.apache.flink.runtime.state.changelog.StateChangelogStorageLoader [] - Creating a changelog storage with name 'memory'.
[2025-11-27T10:41:20.537+0000] {subprocess.py:93} INFO - 2025-11-27 10:41:20,192 INFO  org.apache.flink.runtime.state.TaskExecutorChannelStateExecutorFactoryManager [] - Creating the channel state executor factory for job id 25f973ea87d59a5a5944154a19bcfe15
[2025-11-27T10:41:20.537+0000] {subprocess.py:93} INFO - 2025-11-27 10:41:20,192 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task Source: taxi_trips[1] -> Calc[2] -> Sink: taxi_trips_dashboard[3] (1/1)#0 (5ccddf79ff5cdefc788dc53d4f3ad620_cbc357ccb763df2852fee8c4fc7d55f2_0_0), deploy into slot with allocation id 666e2ff1d9045a04f6410592e47a6b57.
[2025-11-27T10:41:20.537+0000] {subprocess.py:93} INFO - 2025-11-27 10:41:20,193 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 666e2ff1d9045a04f6410592e47a6b57.
[2025-11-27T10:41:20.538+0000] {subprocess.py:93} INFO - 2025-11-27 10:41:20,193 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: taxi_trips[1] -> Calc[2] -> Sink: taxi_trips_dashboard[3] (1/1)#0 (5ccddf79ff5cdefc788dc53d4f3ad620_cbc357ccb763df2852fee8c4fc7d55f2_0_0) switched from CREATED to DEPLOYING.
[2025-11-27T10:41:20.538+0000] {subprocess.py:93} INFO - 2025-11-27 10:41:20,194 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task Source: taxi_trips[1] -> Calc[2] -> Sink: taxi_trips_dashboard[3] (1/1)#0 (5ccddf79ff5cdefc788dc53d4f3ad620_cbc357ccb763df2852fee8c4fc7d55f2_0_0) [DEPLOYING].
[2025-11-27T10:41:20.538+0000] {subprocess.py:93} INFO - 2025-11-27 10:41:20,196 INFO  org.apache.flink.runtime.blob.BlobClient                     [] - Downloading 25f973ea87d59a5a5944154a19bcfe15/p-018b5f856d40a14eb89c4049a445d0425ad38f05-3e7c3650aa441a4ce2f80d705cb2aaaa from flink-jobmanager/172.18.0.6:6124
[2025-11-27T10:41:20.538+0000] {subprocess.py:93} INFO - 2025-11-27 10:41:20,463 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@5601e3bb
[2025-11-27T10:41:20.538+0000] {subprocess.py:93} INFO - 2025-11-27 10:41:20,463 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
[2025-11-27T10:41:20.538+0000] {subprocess.py:93} INFO - 2025-11-27 10:41:20,464 INFO  org.apache.flink.streaming.runtime.tasks.StreamTask          [] - Checkpoint storage is set to 'jobmanager'
[2025-11-27T10:41:20.538+0000] {subprocess.py:93} INFO - 2025-11-27 10:41:20,464 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: taxi_trips[1] -> Calc[2] -> Sink: taxi_trips_dashboard[3] (1/1)#0 (5ccddf79ff5cdefc788dc53d4f3ad620_cbc357ccb763df2852fee8c4fc7d55f2_0_0) switched from DEPLOYING to INITIALIZING.
[2025-11-27T10:41:20.539+0000] {subprocess.py:93} INFO - ----------------------------------------
[2025-11-27T10:41:20.539+0000] {subprocess.py:93} INFO - 
[2025-11-27T10:41:20.539+0000] {subprocess.py:93} INFO - Troubleshooting:
[2025-11-27T10:41:20.539+0000] {subprocess.py:93} INFO - 1. Check if Kafka has messages: docker exec kafka kafka-console-consumer --bootstrap-server localhost:9092 --topic s3-taxi-trips --from-beginning --max-messages 1
[2025-11-27T10:41:20.539+0000] {subprocess.py:93} INFO - 2. Check Flink UI: http://localhost:8081
[2025-11-27T10:41:20.539+0000] {subprocess.py:93} INFO - 3. Verify connectors are available in /opt/flink/usrlib/
[2025-11-27T10:41:20.540+0000] {subprocess.py:97} INFO - Command exited with return code 31
[2025-11-27T10:41:20.558+0000] {taskinstance.py:1935} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/bash.py", line 210, in execute
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 31.
[2025-11-27T10:41:20.563+0000] {taskinstance.py:1398} INFO - Marking task as FAILED. dag_id=taxi_streaming_pipeline, task_id=run_flink_job, execution_date=20251127T103950, start_date=20251127T104059, end_date=20251127T104120
[2025-11-27T10:41:20.584+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 193 for task run_flink_job (Bash command failed. The command returned a non-zero exit code 31.; 824)
[2025-11-27T10:41:20.599+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2025-11-27T10:41:20.646+0000] {taskinstance.py:2776} INFO - 0 downstream tasks scheduled from follow-on schedule check
