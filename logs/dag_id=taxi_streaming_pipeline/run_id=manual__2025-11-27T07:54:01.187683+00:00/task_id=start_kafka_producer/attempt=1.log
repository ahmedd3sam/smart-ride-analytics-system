[2025-11-27T07:54:13.202+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: taxi_streaming_pipeline.start_kafka_producer manual__2025-11-27T07:54:01.187683+00:00 [queued]>
[2025-11-27T07:54:13.220+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: taxi_streaming_pipeline.start_kafka_producer manual__2025-11-27T07:54:01.187683+00:00 [queued]>
[2025-11-27T07:54:13.220+0000] {taskinstance.py:1359} INFO - Starting attempt 1 of 2
[2025-11-27T07:54:13.235+0000] {taskinstance.py:1380} INFO - Executing <Task(BashOperator): start_kafka_producer> on 2025-11-27 07:54:01.187683+00:00
[2025-11-27T07:54:13.240+0000] {standard_task_runner.py:57} INFO - Started process 181 to run task
[2025-11-27T07:54:13.244+0000] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'taxi_streaming_pipeline', 'start_kafka_producer', 'manual__2025-11-27T07:54:01.187683+00:00', '--job-id', '35', '--raw', '--subdir', 'DAGS_FOLDER/taxi_pipeline_dag.py', '--cfg-path', '/tmp/tmpngllpafv']
[2025-11-27T07:54:13.246+0000] {standard_task_runner.py:85} INFO - Job 35: Subtask start_kafka_producer
[2025-11-27T07:54:13.319+0000] {task_command.py:415} INFO - Running <TaskInstance: taxi_streaming_pipeline.start_kafka_producer manual__2025-11-27T07:54:01.187683+00:00 [running]> on host d629b7fb5869
[2025-11-27T07:54:13.369+0000] {abstractoperator.py:709} ERROR - Exception rendering Jinja template for task 'start_kafka_producer', field 'bash_command'. Template: '\n        echo "Starting S3 to Kafka Producer in s3-producer container..."\n        \n        # Check if docker command is available and has socket access\n        DOCKER_AVAILABLE=0\n        if command -v docker &> /dev/null; then\n            if docker ps &> /dev/null; then\n                DOCKER_AVAILABLE=1\n                echo "Docker is available and accessible"\n            else\n                echo "Docker command exists but socket is not accessible"\n            fi\n        fi\n        \n        if [ "$DOCKER_AVAILABLE" = "1" ]; then\n            # Docker is available - use docker commands\n            CONTAINER_EXISTS=$(docker ps -a --format \'{{.Names}}\' | grep -c \'^s3-producer$\' || echo "0")\n            CONTAINER_RUNNING=$(docker ps --format \'{{.Names}}\' | grep -c \'^s3-producer$\' || echo "0")\n            \n            if [ "$CONTAINER_RUNNING" = "1" ]; then\n                echo "s3-producer container is already running. Executing producer script..."\n                docker exec s3-producer python3 /app/s3_to_kafka.py\n                EXIT_CODE=$?\n            elif [ "$CONTAINER_EXISTS" = "1" ]; then\n                echo "s3-producer container exists but is stopped. Starting container..."\n                docker start s3-producer\n                echo "Waiting for container to complete..."\n                EXIT_CODE=$(docker wait s3-producer)\n                echo "Container exited with code: $EXIT_CODE"\n            else\n                echo "s3-producer container does not exist. Creating and starting..."\n                # Try docker-compose first, then docker run\n                if command -v docker-compose &> /dev/null; then\n                    cd /opt/airflow/dags/.. 2>/dev/null || cd /root/s3-kafka-flink-pipeline 2>/dev/null || cd $HOME/s3-kafka-flink-pipeline 2>/dev/null || pwd\n                    docker-compose up -d s3-producer\n                    sleep 5\n                    EXIT_CODE=$(docker wait s3-producer || echo "0")\n                else\n                    echo "Error: s3-producer container not found. Please start it manually."\n                    exit 1\n                fi\n            fi\n        else\n            # Docker not available - assume container is managed externally\n            echo "Docker not available in Airflow container."\n            echo "Assuming s3-producer container is managed by docker-compose."\n            echo "If the container is running, the producer should execute automatically."\n            echo "To start manually, run: docker-compose up -d s3-producer"\n            echo "Waiting 30 seconds for producer to complete..."\n            sleep 30\n            EXIT_CODE=0\n        fi\n        \n        if [ "$EXIT_CODE" != "0" ] && [ "$DOCKER_AVAILABLE" = "1" ]; then\n            echo "Warning: Producer exited with code $EXIT_CODE"\n            echo "Check logs with: docker logs s3-producer"\n        fi\n        \n        echo "Producer execution process completed"\n        '
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/abstractoperator.py", line 701, in _do_render_template_fields
    rendered_content = self.render_template(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/template/templater.py", line 156, in render_template
    template = jinja_env.from_string(value)
  File "/home/airflow/.local/lib/python3.8/site-packages/jinja2/environment.py", line 1105, in from_string
    return cls.from_code(self, self.compile(source), gs, None)
  File "/home/airflow/.local/lib/python3.8/site-packages/jinja2/environment.py", line 768, in compile
    self.handle_exception(source=source_hint)
  File "/home/airflow/.local/lib/python3.8/site-packages/jinja2/environment.py", line 936, in handle_exception
    raise rewrite_traceback_stack(source=source)
  File "<unknown>", line 17, in template
jinja2.exceptions.TemplateSyntaxError: unexpected '.'
[2025-11-27T07:54:13.370+0000] {taskinstance.py:1935} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1516, in _run_raw_task
    self._execute_task_with_callbacks(context, test_mode, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1645, in _execute_task_with_callbacks
    task_orig = self.render_templates(context=context)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 2283, in render_templates
    original_task.render_template_fields(context)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 1245, in render_template_fields
    self._do_render_template_fields(self, self.template_fields, context, jinja_env, set())
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/abstractoperator.py", line 701, in _do_render_template_fields
    rendered_content = self.render_template(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/template/templater.py", line 156, in render_template
    template = jinja_env.from_string(value)
  File "/home/airflow/.local/lib/python3.8/site-packages/jinja2/environment.py", line 1105, in from_string
    return cls.from_code(self, self.compile(source), gs, None)
  File "/home/airflow/.local/lib/python3.8/site-packages/jinja2/environment.py", line 768, in compile
    self.handle_exception(source=source_hint)
  File "/home/airflow/.local/lib/python3.8/site-packages/jinja2/environment.py", line 936, in handle_exception
    raise rewrite_traceback_stack(source=source)
  File "<unknown>", line 17, in template
jinja2.exceptions.TemplateSyntaxError: unexpected '.'
[2025-11-27T07:54:13.384+0000] {taskinstance.py:1398} INFO - Marking task as UP_FOR_RETRY. dag_id=taxi_streaming_pipeline, task_id=start_kafka_producer, execution_date=20251127T075401, start_date=20251127T075413, end_date=20251127T075413
[2025-11-27T07:54:13.396+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 35 for task start_kafka_producer (unexpected '.'; 181)
[2025-11-27T07:54:13.417+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2025-11-27T07:54:13.439+0000] {taskinstance.py:2776} INFO - 0 downstream tasks scheduled from follow-on schedule check
